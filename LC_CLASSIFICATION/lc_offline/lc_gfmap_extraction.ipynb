{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5563cc52",
   "metadata": {},
   "source": [
    "# Land Cover Mapping Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e65275",
   "metadata": {},
   "source": [
    "This notebook serves as an example of how to use GFMap and openEO to extract point features for training a machine learning model to do Land Cover Mapping. \n",
    "\n",
    "The example uses the following steps:\n",
    "- Load the labelled points and ditribute them into spatial hexagons.\n",
    "- Define the pre-processing steps for extracting the features from Sentinel-1 and Sentinel-2 data.\n",
    "- Set-up the Sentinel-1 and Sentinel-2 fetchers with GFMap and launch the openEO jobs to fetch the data.\n",
    "- Combine the results from all the batch jobs into one dataframe.\n",
    "- Train a random forrest classifier using the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40d694e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import openeo\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geojson\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "from openeo_gfmap.manager import _log\n",
    "from openeo_gfmap import TemporalContext, Backend, BackendContext, FetchType\n",
    "from openeo_gfmap.manager.job_splitters import split_job_hex\n",
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.manager import _log\n",
    "from openeo_gfmap.backend import cdse_connection, vito_connection\n",
    "from openeo_gfmap.fetching import build_sentinel2_l2a_extractor, build_sentinel1_grd_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3d175b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "_log.setLevel(logging.INFO)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "_log.addHandler(stream_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s|%(name)s|%(levelname)s:  %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Exclude the other loggers from other libraries\n",
    "class MyLoggerFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name == _log.name\n",
    "\n",
    "stream_handler.addFilter(MyLoggerFilter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c5726",
   "metadata": {},
   "source": [
    "## Distribute labelled points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8aa80e",
   "metadata": {},
   "source": [
    "First, we load in a dataset with target labels. In order for the model to work, the target labels need to be integers. Also, we extract some target points from the target polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1db8d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digesting resources\\AT3304000_LC_REF.gpkg\n",
      "Digesting resources\\BE34057C0_LC_REF.gpkg\n",
      "Digesting resources\\BG0000151_LC_REF.gpkg\n",
      "Digesting resources\\CZ0314123_LC_REF.gpkg\n",
      "Digesting resources\\ES0000022_LC_REF.gpkg\n",
      "Digesting resources\\ES4110114_LC_REF.gpkg\n",
      "Digesting resources\\ES6110005_LC_REF.gpkg\n",
      "Digesting resources\\FR2400534_LC_REF.gpkg\n",
      "Digesting resources\\FR2500088_LC_REF.gpkg\n",
      "Digesting resources\\HUBN20034_LC_REF.gpkg\n",
      "Digesting resources\\HUHN20002_LC_REF.gpkg\n",
      "Digesting resources\\LV0536600_LC_REF.gpkg\n",
      "Digesting resources\\ROSCI0227_LC_REF.gpkg\n",
      "Digesting resources\\SE0540063_LC_REF.gpkg\n",
      "Digesting resources\\SKUEV0307_LC_REF.gpkg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>CODE_1_18</th>\n",
       "      <th>CODE_2_18</th>\n",
       "      <th>CODE_3_18</th>\n",
       "      <th>CODE_4_18</th>\n",
       "      <th>geometry</th>\n",
       "      <th>REF_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>004_1154542</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>121</td>\n",
       "      <td>1210</td>\n",
       "      <td>POINT (4441479.644 2719309.820)</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004_1159640</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>321</td>\n",
       "      <td>3210</td>\n",
       "      <td>POINT (4455579.832 2723493.083)</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004_1161222</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>811</td>\n",
       "      <td>8110</td>\n",
       "      <td>POINT (4451985.264 2723915.898)</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004_1161915</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>811</td>\n",
       "      <td>8110</td>\n",
       "      <td>POINT (4428891.549 2721245.698)</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004_1162261</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>1220</td>\n",
       "      <td>POINT (4409755.801 2720635.752)</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86489</th>\n",
       "      <td>004_377885</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>311</td>\n",
       "      <td>3110</td>\n",
       "      <td>POINT (5031171.347 2932261.964)</td>\n",
       "      <td>VALI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86490</th>\n",
       "      <td>004_377894</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>410</td>\n",
       "      <td>4100</td>\n",
       "      <td>POINT (5032675.904 2932251.885)</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86491</th>\n",
       "      <td>004_377895</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>211</td>\n",
       "      <td>2110</td>\n",
       "      <td>POINT (5031120.057 2932245.202)</td>\n",
       "      <td>VALI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86492</th>\n",
       "      <td>004_377898</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>410</td>\n",
       "      <td>4100</td>\n",
       "      <td>POINT (5036328.995 2932249.014)</td>\n",
       "      <td>VALI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86493</th>\n",
       "      <td>004_377927</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>340</td>\n",
       "      <td>3400</td>\n",
       "      <td>POINT (5034024.265 2932200.115)</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86494 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               UID  CODE_1_18  CODE_2_18  CODE_3_18  CODE_4_18  \\\n",
       "0      004_1154542          1         12        121       1210   \n",
       "1      004_1159640          3         32        321       3210   \n",
       "2      004_1161222          8         81        811       8110   \n",
       "3      004_1161915          8         81        811       8110   \n",
       "4      004_1162261          1         12        122       1220   \n",
       "...            ...        ...        ...        ...        ...   \n",
       "86489   004_377885          3         31        311       3110   \n",
       "86490   004_377894          4         41        410       4100   \n",
       "86491   004_377895          2         21        211       2110   \n",
       "86492   004_377898          4         41        410       4100   \n",
       "86493   004_377927          3         34        340       3400   \n",
       "\n",
       "                              geometry REF_TYPE  \n",
       "0      POINT (4441479.644 2719309.820)     NONE  \n",
       "1      POINT (4455579.832 2723493.083)    TRAIN  \n",
       "2      POINT (4451985.264 2723915.898)    TRAIN  \n",
       "3      POINT (4428891.549 2721245.698)    TRAIN  \n",
       "4      POINT (4409755.801 2720635.752)     NONE  \n",
       "...                                ...      ...  \n",
       "86489  POINT (5031171.347 2932261.964)     VALI  \n",
       "86490  POINT (5032675.904 2932251.885)    TRAIN  \n",
       "86491  POINT (5031120.057 2932245.202)     VALI  \n",
       "86492  POINT (5036328.995 2932249.014)     VALI  \n",
       "86493  POINT (5034024.265 2932200.115)     NONE  \n",
       "\n",
       "[86494 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource_folder = Path(\"resources\")\n",
    "YEAR = 2018\n",
    "\n",
    "input_gpkg = gpd.GeoDataFrame()\n",
    "for file in resource_folder.glob(\"*.gpkg\"):\n",
    "    print(\"Digesting\", file)\n",
    "    input_gpkg = pd.concat([input_gpkg, gpd.read_file(file)], ignore_index=True, sort=False, copy=False)\n",
    "\n",
    "input_gpkg[\"geometry\"] = input_gpkg[\"geometry\"].apply(lambda x: x.centroid)\n",
    "input_gpkg = input_gpkg[['UID',\"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\",'geometry', 'REF_TYPE']]\n",
    "input_gpkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9b9d6",
   "metadata": {},
   "source": [
    "To extract the target point features, we use GFMap to distribute the target points over multiple hexagons. Each hexagon extraction will be performed in a separate openeo job. \n",
    "Splitting up jobs is necessary because processing a large area in one job would cause memory issues.\n",
    "\n",
    "We use `split_job_hex` for distributing the target points over multiple hexagons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd6a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_split = split_job_hex(input_gpkg, max_points=50, grid_resolution=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a66e0",
   "metadata": {},
   "source": [
    "We then create a dataframe where each row represents a single hexagon, and thus batch_job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec1ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_dataframe(split_jobs: List[gpd.GeoDataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the split jobs, containg all the necessary information to run the job.\"\"\"\n",
    "    rows = []\n",
    "    for job in split_jobs:\n",
    "        start_date = datetime.datetime(YEAR, 1, 1)\n",
    "        end_date = datetime.datetime(YEAR, 12, 31)\n",
    "        rows.append(pd.Series({\n",
    "            'out_prefix': 'S1S2-stats',\n",
    "            'out_extension': '.csv',\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'geometry': job.to_json()\n",
    "        }))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe(input_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb92f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = job_df.head(1) # testing: only run one job for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafc7d2",
   "metadata": {},
   "source": [
    "## Define feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597df85",
   "metadata": {},
   "source": [
    "Next, we will define wich features we want to extract from openeo.\n",
    "\n",
    "First we define the process graph, except the actual loading of a collection. This will be done by using the GFMap specific methods.\n",
    "\n",
    "The preprocessing is contained in a [seperate .py file](./features.py) so we can use it for inference later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86454dda",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from features import preprocess_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a246c17",
   "metadata": {},
   "source": [
    "## Fetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe7fab",
   "metadata": {},
   "source": [
    "### Set-up the Sentinel-1 and Sentinel-2 fetchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4048e",
   "metadata": {},
   "source": [
    "Next we use the extractor methods of GFMap to load the collection. Using these methods allows the backend independant loading of collections (e.g. wether or not we still have to calculate the backscatter on S1 data or not).\n",
    "\n",
    "The loaded collections are pre-processed and then aggregated for the target points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8885b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentinel2_collection(\n",
    "        row : pd.Series,\n",
    "        connection: openeo.DataCube,\n",
    "        geometry: geojson.FeatureCollection\n",
    "    )-> openeo.DataCube:\n",
    "    bands = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"SCL\"]\n",
    "    bands_with_platform = [\"S2-L2A-\" + band for band in bands]\n",
    "\n",
    "    extraction_parameters = {\n",
    "        \"load_collection\": {\n",
    "            \"eo:cloud_cover\": lambda val: val <= 80.0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    extractor = build_sentinel2_l2a_extractor(\n",
    "        backend_context=BackendContext(Backend(row.backend_name)),\n",
    "        bands=bands_with_platform,\n",
    "        fetch_type=FetchType.POINT,\n",
    "        **extraction_parameters\n",
    "    )\n",
    "\n",
    "    temporal_context = TemporalContext(row.start_date, row.end_date)\n",
    "\n",
    "    s2 = extractor.get_cube(connection, geometry, temporal_context)\n",
    "    s2 = s2.rename_labels(\"bands\", bands)\n",
    "    return s2\n",
    "\n",
    "def sentinel1_collection(\n",
    "        row: pd.Series,\n",
    "        connection : openeo.DataCube,\n",
    "        geometry: geojson.FeatureCollection,\n",
    "    )-> openeo.DataCube:\n",
    "    bands = [\"VH\", \"VV\"]\n",
    "    bands_with_platform = [\"S1-SIGMA0-\" + band for band in bands]\n",
    "\n",
    "    extractor = build_sentinel1_grd_extractor(\n",
    "        backend_context=BackendContext(Backend(row.backend_name)),\n",
    "        bands=bands_with_platform,\n",
    "        fetch_type=FetchType.POINT,\n",
    "    )\n",
    "\n",
    "    temporal_context = TemporalContext(row.start_date, row.end_date)\n",
    "\n",
    "    s1 = extractor.get_cube(connection, geometry, temporal_context)\n",
    "    s1 = s1.rename_labels(\"bands\", bands)\n",
    "    return s1\n",
    "\n",
    "def load_lc_features(\n",
    "    row: pd.Series,\n",
    "    connection : openeo.DataCube,\n",
    "    **kwargs\n",
    "):\n",
    "    geometry = geojson.loads(row.geometry)\n",
    "    \n",
    "    s2_collection = sentinel2_collection(\n",
    "        row=row,\n",
    "        connection=connection,\n",
    "        geometry=geometry\n",
    "    )\n",
    "\n",
    "    s1_collection = sentinel1_collection(\n",
    "        row=row,\n",
    "        connection=connection,\n",
    "        geometry=geometry\n",
    "    )\n",
    "\n",
    "    features = preprocess_features(s2_collection, s1_collection)\n",
    "\n",
    "    # Currently, aggregate_spatial and vectorcubes do not keep the band names, so we'll need to rename them later on\n",
    "    global final_band_names\n",
    "    final_band_names = [b.name for b in features.metadata.band_dimension.bands]\n",
    "\n",
    "    features = features.aggregate_spatial(geometry, reducer=\"median\")\n",
    "    \n",
    "    job_options = {\n",
    "        \"executor-memory\": \"3G\", # Increase this value if a job fails due to memory issues\n",
    "        \"executor-memoryOverhead\": \"2G\",\n",
    "        \"soft-errors\": True\n",
    "    }\n",
    "\n",
    "    return features.create_job(\n",
    "        out_format=\"csv\",\n",
    "        title=f\"GFMAP_Extraction_{geometry.features[0].properties['h3index']}\",\n",
    "        job_options=job_options,\n",
    "    )\n",
    "\n",
    "# Global variable to store the final band names\n",
    "final_band_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d48fd",
   "metadata": {},
   "source": [
    "### Launch the openEO jobs to fetch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6dcc6",
   "metadata": {},
   "source": [
    "In order to launch the jobs, we have to define a function that fill determine the outputfile name and create the job manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716ae4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_path(\n",
    "    root_folder: Path,\n",
    "    geometry_index: int,\n",
    "    row: pd.Series\n",
    ") -> Path:\n",
    "    features = geojson.loads(row.geometry)\n",
    "    h3index = features[geometry_index].properties['h3index']\n",
    "    result = root_folder / f\"{row.out_prefix}_{h3index}_{geometry_index}{row.out_extension}\"\n",
    "    print(\"output_path:\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d289b7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestr: 20240322-16h54\n"
     ]
    }
   ],
   "source": [
    "base_output_path = Path(\"output\")\n",
    "base_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "timenow = datetime.datetime.now()\n",
    "timestr = timenow.strftime(\"%Y%m%d-%Hh%M\")\n",
    "print(f\"Timestr: {timestr}\")\n",
    "tracking_file = base_output_path / f\"tracking_{timestr}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f7ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = GFMAPJobManager(\n",
    "    output_dir=base_output_path / timestr,\n",
    "    output_path_generator=generate_output_path,\n",
    "    poll_sleep=60,\n",
    "    n_threads=1,\n",
    "    collection_id=\"LC_feature_extraction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "628675bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.add_backend(Backend.CDSE, cdse_connection, parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962704d4",
   "metadata": {},
   "source": [
    "We then run the prepared jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f328baa5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 16:54:52,958|openeo_gfmap.manager|INFO:  Starting job manager using 1 worker threads.\n",
      "2024-03-22 16:54:52,961|openeo_gfmap.manager|INFO:  Workers started, creating and running jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n",
      "DataCube(<PGNode 'dimension_labels' at 0x2252402c8c0>)\n",
      "DataCube(<PGNode 'dimension_labels' at 0x2251fe48140>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 17:14:33,102|openeo_gfmap.manager|INFO:  Job j-24032263ac114973a8df2544441484e4 finished successfully, queueing on_job_done...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_path: output\\20240322-16h54\\S1S2-stats_8408b09ffffffff_0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 17:14:36,961|openeo_gfmap.manager|INFO:  Added 0 items to the STAC collection.\n",
      "2024-03-22 17:14:36,963|openeo_gfmap.manager|INFO:  Job j-24032263ac114973a8df2544441484e4 and post job action finished successfully.\n"
     ]
    }
   ],
   "source": [
    "manager.run_jobs(\n",
    "    job_df,\n",
    "    load_lc_features,\n",
    "    tracking_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f621d",
   "metadata": {},
   "source": [
    "## Combine the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc57588",
   "metadata": {},
   "source": [
    "We combine all the different extractions into one dataframe to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d906acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run these lines to post-process older results\n",
    "timestr = \"20240318-20h30\"\n",
    "tracking_file = base_output_path / f\"tracking_{timestr}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "263f5291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VERHAERV\\AppData\\Local\\Temp\\ipykernel_25584\\3754540950.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, stats_df])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tracker_df = pd.read_csv(tracking_file)\n",
    "df = pd.DataFrame(columns = final_band_names + [\"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\", 'geometry'])\n",
    "\n",
    "for index, row in tracker_df.iterrows():\n",
    "    if row.status == \"finished\":\n",
    "        try:\n",
    "            # Get the target and geometry from the input\n",
    "            geometry = gpd.read_file(row.geometry)\n",
    "            geometry['id'] = geometry['id'].astype(int)\n",
    "            h3index = geometry.iloc[0]['h3index']\n",
    "            filename = f\"S1S2-stats_{h3index}_0.csv\"\n",
    "            target_df = geometry[['id', \"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\", 'geometry']]\n",
    "\n",
    "            # Read the stats\n",
    "            stats_df = pd.read_csv(base_output_path/timestr/filename)\n",
    "            stats_df.columns = ['id'] + final_band_names\n",
    "\n",
    "            # Merge the target and geometry with the stats\n",
    "            stats_df = stats_df.merge(target_df, how='left', on='id')\n",
    "            stats_df = stats_df.drop(columns=['id'])\n",
    "\n",
    "            # Append to the dataframe\n",
    "            df = pd.concat([df, stats_df])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {filename}\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7051082",
   "metadata": {},
   "source": [
    "Here we filter out features that contain NaN values. These often correspond to the months January and December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43034a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop NA columns\n",
    "# nan_columns = df.columns[df.isna().any()].tolist()\n",
    "# print(f\"Dropping columns containing NaN: {nan_columns}\")\n",
    "# df.drop(nan_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e3e4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(base_output_path / timestr / \"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce86796",
   "metadata": {},
   "source": [
    "## Training and saving a random forrest model\n",
    "The Following is just an example of local training a random forrest.\n",
    "\n",
    "The model is converted to an ONNX model and saved. ONNX is a format to store machine learning models in a standardized way. This allows us to use the model in other applications, such as the openEO backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e531f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from skl2onnx import to_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf46d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_output_path / timestr / \"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f4fb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\", 'geometry'])\n",
    "X = X.astype(np.float32) # convert to float32 to allow ONNX conversion later on\n",
    "y = df['CODE_1_18'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ab4fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features=y.unique().size, random_state=42)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy on test set: \"+str(accuracy_score(y_test,y_pred))[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbb3c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = base_output_path / \"models\"\n",
    "model_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "onnx = to_onnx(model=rf, name=\"random_forest\", X=X_train.values)\n",
    "\n",
    "with open(base_output_path / \"models\" / \"random_forest.onnx\", \"wb\") as f:\n",
    "    f.write(onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce06488",
   "metadata": {},
   "source": [
    "See the [inference notebook](./lc_inference.ipynb) for an example of how to use the model for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openEO Python Client",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
